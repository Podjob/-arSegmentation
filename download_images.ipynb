{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Скачиваем фотографии машин из интернета для теста модели"
      ],
      "metadata": {
        "id": "kIYwOEPIkxcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/QianyanTech/Image-Downloader\n",
        "!pip install -r Image-Downloader/requirements.txt"
      ],
      "metadata": {
        "id": "FyBC2ESrt-1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скрип питона для скачки фотографий\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "import imghdr\n",
        "import concurrent.futures\n",
        "import requests\n",
        "import socket\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "from urllib.parse import unquote, quote\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import requests\n",
        "from concurrent import futures\n",
        "\n",
        "g_headers = {\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    \"Proxy-Connection\": \"keep-alive\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, sdch\",\n",
        "}\n",
        "\n",
        "if getattr(sys, 'frozen', False):\n",
        "    bundle_dir = sys._MEIPASS\n",
        "else:\n",
        "    bundle_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "\n",
        "def my_print(msg, quiet=False):\n",
        "    if not quiet:\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "def google_gen_query_url(keywords, face_only=False, safe_mode=False, image_type=None, color=None):\n",
        "    base_url = \"https://www.google.com/search?tbm=isch&hl=en\"\n",
        "    keywords_str = \"&q=\" + quote(keywords)\n",
        "    query_url = base_url + keywords_str\n",
        "\n",
        "    if safe_mode is True:\n",
        "        query_url += \"&safe=on\"\n",
        "    else:\n",
        "        query_url += \"&safe=off\"\n",
        "\n",
        "    filter_url = \"&tbs=\"\n",
        "\n",
        "    if color is not None:\n",
        "        if color == \"bw\":\n",
        "            filter_url += \"ic:gray%2C\"\n",
        "        else:\n",
        "            filter_url += \"ic:specific%2Cisc:{}%2C\".format(color.lower())\n",
        "\n",
        "    if image_type is not None:\n",
        "        if image_type.lower() == \"linedrawing\":\n",
        "            image_type = \"lineart\"\n",
        "        filter_url += \"itp:{}\".format(image_type)\n",
        "\n",
        "    if face_only is True:\n",
        "        filter_url += \"itp:face\"\n",
        "\n",
        "    query_url += filter_url\n",
        "    return query_url\n",
        "\n",
        "\n",
        "def google_image_url_from_webpage(driver, max_number, quiet=False):\n",
        "    thumb_elements_old = []\n",
        "    thumb_elements = []\n",
        "    while True:\n",
        "        try:\n",
        "            thumb_elements = driver.find_elements(By.CLASS_NAME, \"rg_i\")\n",
        "            my_print(\"Find {} images.\".format(len(thumb_elements)), quiet)\n",
        "            if len(thumb_elements) >= max_number:\n",
        "                break\n",
        "            if len(thumb_elements) == len(thumb_elements_old):\n",
        "                break\n",
        "            thumb_elements_old = thumb_elements\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            time.sleep(2)\n",
        "            show_more = driver.find_elements(By.CLASS_NAME, \"mye4qd\")\n",
        "            if len(show_more) == 1 and show_more[0].is_displayed() and show_more[0].is_enabled():\n",
        "                my_print(\"Click show_more button.\", quiet)\n",
        "                show_more[0].click()\n",
        "            time.sleep(3)\n",
        "        except Exception as e:\n",
        "            print(\"Exception \", e)\n",
        "            pass\n",
        "\n",
        "    if len(thumb_elements) == 0:\n",
        "        return []\n",
        "\n",
        "    my_print(\"Click on each thumbnail image to get image url, may take a moment ...\", quiet)\n",
        "\n",
        "    retry_click = []\n",
        "    for i, elem in enumerate(thumb_elements):\n",
        "        try:\n",
        "            if i != 0 and i % 50 == 0:\n",
        "                my_print(\"{} thumbnail clicked.\".format(i), quiet)\n",
        "            if not elem.is_displayed() or not elem.is_enabled():\n",
        "                retry_click.append(elem)\n",
        "                continue\n",
        "            elem.click()\n",
        "        except Exception as e:\n",
        "            print(\"Error while clicking in thumbnail:\", e)\n",
        "            retry_click.append(elem)\n",
        "\n",
        "    if len(retry_click) > 0:\n",
        "        my_print(\"Retry some failed clicks ...\", quiet)\n",
        "        for elem in retry_click:\n",
        "            try:\n",
        "                if elem.is_displayed() and elem.is_enabled():\n",
        "                    elem.click()\n",
        "            except Exception as e:\n",
        "                print(\"Error while retrying click:\", e)\n",
        "\n",
        "    image_elements = driver.find_elements(By.CLASS_NAME, \"islib\")\n",
        "    image_urls = list()\n",
        "    url_pattern = r\"imgurl=\\S*&amp;imgrefurl\"\n",
        "\n",
        "    for image_element in image_elements[:max_number]:\n",
        "        outer_html = image_element.get_attribute(\"outerHTML\")\n",
        "        re_group = re.search(url_pattern, outer_html)\n",
        "        if re_group is not None:\n",
        "            image_url = unquote(re_group.group()[7:-14])\n",
        "            image_urls.append(image_url)\n",
        "    return image_urls\n",
        "\n",
        "\n",
        "def bing_gen_query_url(keywords, face_only=False, safe_mode=False, image_type=None, color=None):\n",
        "    base_url = \"https://www.bing.com/images/search?\"\n",
        "    keywords_str = \"&q=\" + quote(keywords)\n",
        "    query_url = base_url + keywords_str\n",
        "    filter_url = \"&qft=\"\n",
        "    if face_only is True:\n",
        "        filter_url += \"+filterui:face-face\"\n",
        "\n",
        "    if image_type is not None:\n",
        "        filter_url += \"+filterui:photo-{}\".format(image_type)\n",
        "\n",
        "    if color is not None:\n",
        "        if color == \"bw\" or color == \"color\":\n",
        "            filter_url += \"+filterui:color2-{}\".format(color.lower())\n",
        "        else:\n",
        "            filter_url += \"+filterui:color2-FGcls_{}\".format(color.upper())\n",
        "\n",
        "    query_url += filter_url\n",
        "\n",
        "    return query_url\n",
        "\n",
        "\n",
        "def bing_image_url_from_webpage(driver):\n",
        "    image_urls = list()\n",
        "\n",
        "    time.sleep(10)\n",
        "    img_count = 0\n",
        "\n",
        "    while True:\n",
        "        image_elements = driver.find_elements(By.CLASS_NAME, \"iusc\")\n",
        "        if len(image_elements) > img_count:\n",
        "            img_count = len(image_elements)\n",
        "            driver.execute_script(\n",
        "                \"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        else:\n",
        "            smb = driver.find_elements(By.CLASS_NAME, \"btn_seemore\")\n",
        "            if len(smb) > 0 and smb[0].is_displayed():\n",
        "                smb[0].click()\n",
        "            else:\n",
        "                break\n",
        "        time.sleep(3)\n",
        "    for image_element in image_elements:\n",
        "        m_json_str = image_element.get_attribute(\"m\")\n",
        "        m_json = json.loads(m_json_str)\n",
        "        image_urls.append(m_json[\"murl\"])\n",
        "    return image_urls\n",
        "\n",
        "def bing_get_image_url_using_api(keywords, max_number=10000, face_only=False,\n",
        "                                 proxy=None, proxy_type=None):\n",
        "    proxies = None\n",
        "    if proxy and proxy_type:\n",
        "        proxies = {\"http\": \"{}://{}\".format(proxy_type, proxy),\n",
        "                   \"https\": \"{}://{}\".format(proxy_type, proxy)}\n",
        "    start = 1\n",
        "    image_urls = []\n",
        "    while start <= max_number:\n",
        "        url = 'https://www.bing.com/images/async?q={}&first={}&count=35'.format(keywords, start)\n",
        "        res = requests.get(url, proxies=proxies, headers=g_headers)\n",
        "        res.encoding = \"utf-8\"\n",
        "        image_urls_batch = re.findall('murl&quot;:&quot;(.*?)&quot;', res.text)\n",
        "        if len(image_urls) > 0 and image_urls_batch[-1] == image_urls[-1]:\n",
        "            break\n",
        "        image_urls += image_urls_batch\n",
        "        start += len(image_urls_batch)\n",
        "    return image_urls\n",
        "\n",
        "baidu_color_code = {\n",
        "    \"white\": 1024, \"bw\": 2048, \"black\": 512, \"pink\": 64, \"blue\": 16, \"red\": 1,\n",
        "    \"yellow\": 2, \"purple\": 32, \"green\": 4, \"teal\": 8, \"orange\": 256, \"brown\": 128\n",
        "}\n",
        "\n",
        "def baidu_gen_query_url(keywords, face_only=False, safe_mode=False, color=None):\n",
        "    base_url = \"https://image.baidu.com/search/index?tn=baiduimage\"\n",
        "    keywords_str = \"&word=\" + quote(keywords)\n",
        "    query_url = base_url + keywords_str\n",
        "    if face_only is True:\n",
        "        query_url += \"&face=1\"\n",
        "    if color is not None:\n",
        "        print(color, baidu_color_code[color.lower()])\n",
        "    if color is not None:\n",
        "        query_url += \"&ic={}\".format(baidu_color_code[color.lower()])\n",
        "    print(query_url)\n",
        "    return query_url\n",
        "\n",
        "\n",
        "def baidu_image_url_from_webpage(driver):\n",
        "    time.sleep(10)\n",
        "    image_elements = driver.find_elements(By.CLASS_NAME, \"imgitem\")\n",
        "    image_urls = list()\n",
        "\n",
        "    for image_element in image_elements:\n",
        "        image_url = image_element.get_attribute(\"data-objurl\")\n",
        "        image_urls.append(image_url)\n",
        "    return image_urls\n",
        "\n",
        "\n",
        "def baidu_get_image_url_using_api(keywords, max_number=10000, face_only=False,\n",
        "                                  proxy=None, proxy_type=None):\n",
        "    def decode_url(url):\n",
        "        in_table = '0123456789abcdefghijklmnopqrstuvw'\n",
        "        out_table = '7dgjmoru140852vsnkheb963wtqplifca'\n",
        "        translate_table = str.maketrans(in_table, out_table)\n",
        "        mapping = {'_z2C$q': ':', '_z&e3B': '.', 'AzdH3F': '/'}\n",
        "        for k, v in mapping.items():\n",
        "            url = url.replace(k, v)\n",
        "        return url.translate(translate_table)\n",
        "\n",
        "    base_url = \"https://image.baidu.com/search/acjson?tn=resultjson_com&ipn=rj&ct=201326592\"\\\n",
        "               \"&lm=7&fp=result&ie=utf-8&oe=utf-8&st=-1\"\n",
        "    keywords_str = \"&word={}&queryWord={}\".format(\n",
        "        quote(keywords), quote(keywords))\n",
        "    query_url = base_url + keywords_str\n",
        "    query_url += \"&face={}\".format(1 if face_only else 0)\n",
        "\n",
        "    init_url = query_url + \"&pn=0&rn=30\"\n",
        "\n",
        "    proxies = None\n",
        "    if proxy and proxy_type:\n",
        "        proxies = {\"http\": \"{}://{}\".format(proxy_type, proxy),\n",
        "                   \"https\": \"{}://{}\".format(proxy_type, proxy)}\n",
        "\n",
        "    res = requests.get(init_url, proxies=proxies, headers=g_headers)\n",
        "    init_json = json.loads(res.text.replace(r\"\\'\", \"\").encode(\"utf-8\"), strict=False)\n",
        "    total_num = init_json['listNum']\n",
        "\n",
        "    target_num = min(max_number, total_num)\n",
        "    crawl_num = min(target_num * 2, total_num)\n",
        "\n",
        "    crawled_urls = list()\n",
        "    batch_size = 30\n",
        "\n",
        "    with futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        future_list = list()\n",
        "\n",
        "        def process_batch(batch_no, batch_size):\n",
        "            image_urls = list()\n",
        "            url = query_url + \\\n",
        "                \"&pn={}&rn={}\".format(batch_no * batch_size, batch_size)\n",
        "            try_time = 0\n",
        "            while True:\n",
        "                try:\n",
        "                    response = requests.get(url, proxies=proxies, headers=g_headers)\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    try_time += 1\n",
        "                    if try_time > 3:\n",
        "                        print(e)\n",
        "                        return image_urls\n",
        "            response.encoding = 'utf-8'\n",
        "            res_json = json.loads(response.text.replace(r\"\\'\", \"\"), strict=False)\n",
        "            for data in res_json['data']:\n",
        "                if 'objURL' in data.keys():\n",
        "                    url = unquote(decode_url(data['objURL']))\n",
        "                    if 'src=' in url:\n",
        "                        url_p1 = url.split('src=')[1]\n",
        "                        url = url_p1.split('&refer=')[0]\n",
        "                    image_urls.append(url)\n",
        "                elif 'replaceUrl' in data.keys() and len(data['replaceUrl']) == 2:\n",
        "                    image_urls.append(data['replaceUrl'][1]['ObjURL'])\n",
        "\n",
        "            return image_urls\n",
        "\n",
        "        for i in range(0, int((crawl_num + batch_size - 1) / batch_size)):\n",
        "            future_list.append(executor.submit(process_batch, i, batch_size))\n",
        "        for future in futures.as_completed(future_list):\n",
        "            if future.exception() is None:\n",
        "                crawled_urls += future.result()\n",
        "            else:\n",
        "                print(future.exception())\n",
        "\n",
        "    return crawled_urls[:min(len(crawled_urls), target_num)]\n",
        "\n",
        "\n",
        "def crawl_image_urls(keywords, engine=\"Google\", max_number=10000,\n",
        "                     face_only=False, safe_mode=False, proxy=None,\n",
        "                     proxy_type=\"http\", quiet=False, browser=\"chrome_headless\", image_type=None, color=None):\n",
        "    my_print(\"\\nScraping From {} Image Search ...\\n\".format(engine), quiet)\n",
        "    my_print(\"Keywords:  \" + keywords, quiet)\n",
        "    if max_number <= 0:\n",
        "        my_print(\"Number:  No limit\", quiet)\n",
        "        max_number = 10000\n",
        "    else:\n",
        "        my_print(\"Number:  {}\".format(max_number), quiet)\n",
        "    my_print(\"Face Only:  {}\".format(str(face_only)), quiet)\n",
        "    my_print(\"Safe Mode:  {}\".format(str(safe_mode)), quiet)\n",
        "\n",
        "    if engine == \"Google\":\n",
        "        query_url = google_gen_query_url(keywords, face_only, safe_mode, image_type, color)\n",
        "    elif engine == \"Bing\":\n",
        "        query_url = bing_gen_query_url(keywords, face_only, safe_mode, image_type, color)\n",
        "    elif engine == \"Baidu\":\n",
        "        query_url = baidu_gen_query_url(keywords, face_only, safe_mode, color)\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    my_print(\"Query URL:  \" + query_url, quiet)\n",
        "\n",
        "    image_urls = []\n",
        "\n",
        "    if browser != \"api\":\n",
        "        browser = str.lower(browser)\n",
        "        chrome_path = shutil.which(\"chromedriver\")\n",
        "        chrome_options = webdriver.ChromeOptions()\n",
        "        if \"headless\" in browser:\n",
        "            chrome_options.add_argument(\"headless\")\n",
        "        if proxy is not None and proxy_type is not None:\n",
        "            chrome_options.add_argument(\"--proxy-server={}://{}\".format(proxy_type, proxy))\n",
        "        driver = webdriver.Chrome(chrome_path, chrome_options=chrome_options)\n",
        "\n",
        "        if engine == \"Google\":\n",
        "            driver.set_window_size(1920, 1080)\n",
        "            driver.get(query_url)\n",
        "            image_urls = google_image_url_from_webpage(driver, max_number, quiet)\n",
        "        elif engine == \"Bing\":\n",
        "            driver.set_window_size(1920, 1080)\n",
        "            driver.get(query_url)\n",
        "            image_urls = bing_image_url_from_webpage(driver)\n",
        "        else:   # Baidu\n",
        "            driver.set_window_size(10000, 7500)\n",
        "            driver.get(query_url)\n",
        "            image_urls = baidu_image_url_from_webpage(driver)\n",
        "        driver.close()\n",
        "    else: # api\n",
        "        if engine == \"Baidu\":\n",
        "            image_urls = baidu_get_image_url_using_api(keywords, max_number=max_number, face_only=face_only,\n",
        "                                                       proxy=proxy, proxy_type=proxy_type)\n",
        "        elif engine == \"Bing\":\n",
        "            image_urls = bing_get_image_url_using_api(keywords, max_number=max_number, face_only=face_only,\n",
        "                                                      proxy=proxy, proxy_type=proxy_type)\n",
        "        else:\n",
        "            my_print(\"Engine {} is not supported on API mode.\".format(engine))\n",
        "\n",
        "    if max_number > len(image_urls):\n",
        "        output_num = len(image_urls)\n",
        "    else:\n",
        "        output_num = max_number\n",
        "\n",
        "    my_print(\"\\n== {0} out of {1} crawled images urls will be used.\\n\".format(\n",
        "        output_num, len(image_urls)), quiet)\n",
        "\n",
        "    return image_urls[0:output_num]\n",
        "\n",
        "headers = {\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    \"Proxy-Connection\": \"keep-alive\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, sdch\",\n",
        "}\n",
        "\n",
        "def download_image(image_url, dst_dir, file_name, timeout=20, proxy_type=None, proxy=None):\n",
        "    proxies = None\n",
        "    if proxy_type is not None:\n",
        "        proxies = {\n",
        "            \"http\": proxy_type + \"://\" + proxy,\n",
        "            \"https\": proxy_type + \"://\" + proxy\n",
        "        }\n",
        "\n",
        "    response = None\n",
        "    file_path = os.path.join(dst_dir, file_name)\n",
        "    try_times = 0\n",
        "    while True:\n",
        "        try:\n",
        "            try_times += 1\n",
        "            response = requests.get(\n",
        "                image_url, headers=headers, timeout=timeout, proxies=proxies)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            response.close()\n",
        "            file_type = imghdr.what(file_path)\n",
        "            if file_type in [\"jpg\", \"jpeg\", \"png\", \"bmp\", \"webp\"]:\n",
        "                new_file_name = \"{}.{}\".format(file_name, file_type)\n",
        "                new_file_path = os.path.join(dst_dir, new_file_name)\n",
        "                shutil.move(file_path, new_file_path)\n",
        "                print(\"## OK:  {}  {}\".format(new_file_name, image_url))\n",
        "            else:\n",
        "                os.remove(file_path)\n",
        "                print(\"## Err: TYPE({})  {}\".format(file_type, image_url))\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if try_times < 3:\n",
        "                continue\n",
        "            if response:\n",
        "                response.close()\n",
        "            print(\"## Fail:  {}  {}\".format(image_url, e.args))\n",
        "            break\n",
        "\n",
        "\n",
        "def download_images(image_urls, dst_dir, file_prefix=\"img\", concurrency=50, timeout=20, proxy_type=None, proxy=None):\n",
        "    socket.setdefaulttimeout(timeout)\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n",
        "        future_list = list()\n",
        "        count = 0\n",
        "        if not os.path.exists(dst_dir):\n",
        "            os.makedirs(dst_dir)\n",
        "        for image_url in image_urls:\n",
        "            file_name = file_prefix + \"_\" + \"%04d\" % count\n",
        "            future_list.append(executor.submit(\n",
        "                download_image, image_url, dst_dir, file_name, timeout, proxy_type, proxy))\n",
        "            count += 1\n",
        "        concurrent.futures.wait(future_list, timeout=180)\n",
        "\n",
        "import chromedriver_autoinstaller\n",
        "\n",
        "def gen_valid_dir_name_for_keywords(keywords):\n",
        "    keep = [\"-\", \"_\", \".\"]\n",
        "    keywords = keywords.replace(\" \", \"_\").replace(\":\", \"-\")\n",
        "    return \"\".join(c for c in keywords if c.isalnum() or c in keep).rstrip()\n",
        "\n",
        "\n",
        "class AppConfig(object):\n",
        "    def __init__(self):\n",
        "        self.engine = \"Google\"\n",
        "\n",
        "        self.driver = \"chrome_headless\"\n",
        "\n",
        "        self.keywords = \"\"\n",
        "\n",
        "        self.max_number = 0\n",
        "\n",
        "        self.face_only = False\n",
        "\n",
        "        self.safe_mode = False\n",
        "\n",
        "        self.proxy_type = None\n",
        "        self.proxy = None\n",
        "\n",
        "        self.num_threads = 50\n",
        "\n",
        "        self.output_dir = \"./output\"\n",
        "\n",
        "    def to_command_paras(self):\n",
        "        str_paras = \"\"\n",
        "\n",
        "        str_paras += ' -e ' + self.engine\n",
        "\n",
        "        str_paras += ' -d ' + self.driver\n",
        "\n",
        "        str_paras += ' -n ' + str(self.max_number)\n",
        "\n",
        "        str_paras += ' -j ' + str(self.num_threads)\n",
        "\n",
        "        str_paras += ' -o \"' + self.output_dir + '/' + \\\n",
        "            gen_valid_dir_name_for_keywords(self.keywords) + '\"'\n",
        "\n",
        "        if self.face_only:\n",
        "            str_paras += ' -F '\n",
        "\n",
        "        if self.safe_mode:\n",
        "            str_paras += ' -S '\n",
        "\n",
        "        if self.proxy_type == \"http\":\n",
        "            str_paras += ' -ph \"' + self.proxy + '\"'\n",
        "        elif self.proxy_type == \"socks5\":\n",
        "            str_paras += ' -ps \"' + self.proxy + '\"'\n",
        "\n",
        "        str_paras += ' \"' + self.keywords + '\"'\n",
        "\n",
        "        return str_paras\n",
        "\n",
        "\n",
        "def gen_keywords_list_from_str(keywords_str, sep=\",\"):\n",
        "    return keywords_str.split(sep)\n",
        "\n",
        "\n",
        "def gen_keywords_list_from_file(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.readlines()\n",
        "\n",
        "def resolve_dependencies(driver=str):\n",
        "    if \"chrome\" in driver:\n",
        "        print(\"Checking Google Chrome and chromedriver ...\")\n",
        "        driver_path = chromedriver_autoinstaller.install()\n",
        "        if not driver_path:\n",
        "            return False\n",
        "        print(\"OK.\")\n",
        "    return True\n",
        "\n",
        "def main(argv):\n",
        "    parser = argparse.ArgumentParser(description=\"Image Downloader\")\n",
        "    parser.add_argument(\"keywords\", type=str,\n",
        "                        help='Keywords to search. (\"in quotes\")')\n",
        "    parser.add_argument(\"--engine\", \"-e\", type=str, default=\"Google\",\n",
        "                        help=\"Image search engine.\", choices=[\"Google\", \"Bing\", \"Baidu\"])\n",
        "    parser.add_argument(\"--driver\", \"-d\", type=str, default=\"chrome_headless\",\n",
        "                        help=\"Image search engine.\", choices=[\"chrome_headless\", \"chrome\", \"api\"])\n",
        "    parser.add_argument(\"--max-number\", \"-n\", type=int, default=100,\n",
        "                        help=\"Max number of images download for the keywords.\")\n",
        "    parser.add_argument(\"--num-threads\", \"-j\", type=int, default=50,\n",
        "                        help=\"Number of threads to concurrently download images.\")\n",
        "    parser.add_argument(\"--timeout\", \"-t\", type=int, default=10,\n",
        "                        help=\"Seconds to timeout when download an image.\")\n",
        "    parser.add_argument(\"--output\", \"-o\", type=str, default=\"./download_images\",\n",
        "                        help=\"Output directory to save downloaded images.\")\n",
        "    parser.add_argument(\"--safe-mode\", \"-S\", action=\"store_true\", default=False,\n",
        "                        help=\"Turn on safe search mode. (Only effective in Google)\")\n",
        "    parser.add_argument(\"--face-only\", \"-F\", action=\"store_true\", default=False,\n",
        "                        help=\"Only search for \")\n",
        "    parser.add_argument(\"--proxy_http\", \"-ph\", type=str, default=None,\n",
        "                        help=\"Set http proxy (e.g. 192.168.0.2:8080)\")\n",
        "    parser.add_argument(\"--proxy_socks5\", \"-ps\", type=str, default=None,\n",
        "                        help=\"Set socks5 proxy (e.g. 192.168.0.2:1080)\")\n",
        "    parser.add_argument(\"--type\", \"-ty\", type=str, default=None,\n",
        "                        help=\"What kinds of images to download.\", choices=[\"clipart\", \"linedrawing\", \"photograph\"])\n",
        "    parser.add_argument(\"--color\", \"-cl\", type=str, default=None,\n",
        "                        help=\"Specify the color of desired images.\")\n",
        "\n",
        "    args = parser.parse_args(args=argv)\n",
        "\n",
        "    proxy_type = None\n",
        "    proxy = None\n",
        "    if args.proxy_http is not None:\n",
        "        proxy_type = \"http\"\n",
        "        proxy = args.proxy_http\n",
        "    elif args.proxy_socks5 is not None:\n",
        "        proxy_type = \"socks5\"\n",
        "        proxy = args.proxy_socks5\n",
        "\n",
        "    if not resolve_dependencies(args.driver):\n",
        "        print(\"Dependencies not resolved, exit.\")\n",
        "        return\n",
        "\n",
        "    crawled_urls = crawl_image_urls(args.keywords,\n",
        "                                            engine=args.engine, max_number=args.max_number,\n",
        "                                            face_only=args.face_only, safe_mode=args.safe_mode,\n",
        "                                            proxy_type=proxy_type, proxy=proxy,\n",
        "                                            browser=args.driver, image_type=args.type, color=args.color)\n",
        "    download_images(image_urls=crawled_urls, dst_dir=args.output,\n",
        "                               concurrency=args.num_threads, timeout=args.timeout,\n",
        "                               proxy_type=proxy_type, proxy=proxy,\n",
        "                               file_prefix=args.engine)\n",
        "\n",
        "    print(\"Finished.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(sys.argv[1:])\n"
      ],
      "metadata": {
        "id": "AzaXSVOoryqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cохраняем скрипт в отдельный файл download.py\n",
        "with open(\"download.py\", 'w') as f:\n",
        "  f.write(In[-2])"
      ],
      "metadata": {
        "id": "uQfL6XUEtqTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подключаем гугл диск\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CbAMV6lnTal0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачиваем фотографии машин сбоку\n",
        "%%shell\n",
        "strings=(\"Toyota\" \"Volkswagen\" \"Ford\" \"Honda\" \"Chevrolet\" \"Hyundai\" \"Nissan\" \"BMW\" \"Mercedes-Benz\" \"Kia\" \"Suzuki\" \"Renault\" \"Peugeot\" \"Daihatsu\" \"Mazda\" \"Subaru\" \"Tata\" \"Volvo\" \"Mitsubishi\")\n",
        "count=1\n",
        "\n",
        "for string in \"${strings[@]}\"; do\n",
        "    python download.py \"$string car side view -front -logo -back\" --engine Bing --driver api --max-number 100 --timeout 2 --output \"/content/drive/My Drive/cars/bing$count\"\n",
        "    ((count++))\n",
        "    sleep 10\n",
        "done"
      ],
      "metadata": {
        "id": "hU_Job8Z5LLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/cars\""
      ],
      "metadata": {
        "id": "uWv9PgD0pecf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выносим фотографии определенного автопроизводителя из подпапки в родительскую папку\n",
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "for dir in */; do\n",
        "    cd \"$dir\"\n",
        "    for file in *; do\n",
        "        if [ -f \"$file\" ]; then\n",
        "            new_filename=\"${dir%/}_$file\"\n",
        "            mv \"$file\" \"../$new_filename\"\n",
        "        fi\n",
        "    done\n",
        "    cd ..\n",
        "done\n",
        "\n",
        "find . -type d -empty -delete"
      ],
      "metadata": {
        "id": "fYiEeUIwpZan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/cars\""
      ],
      "metadata": {
        "id": "D6kxWt4aqX_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Устанавливаем программу для удаления дубликатов\n",
        "%%shell\n",
        "apt-get install fdupes"
      ],
      "metadata": {
        "id": "TBzIcZuk4naW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаляем дубликаты картинок\n",
        "!fdupes -dN \"/content/drive/My Drive/cars\""
      ],
      "metadata": {
        "id": "aWbqOFZX5qnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Переименовываем файлы в порядковые цифры\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "def shuffle_and_rename_files(directory):\n",
        "    files = os.listdir(directory)\n",
        "\n",
        "    random.shuffle(files)\n",
        "\n",
        "    for i, file in enumerate(files, start=1):\n",
        "        old_path = os.path.join(directory, file)\n",
        "        file_extension = pathlib.Path(file).suffix\n",
        "        new_name = f\"{i}{file_extension}\"\n",
        "        new_path = os.path.join(directory, new_name)\n",
        "        os.rename(old_path, new_path)\n",
        "\n",
        "directory_path = \".\"\n",
        "shuffle_and_rename_files(directory_path)"
      ],
      "metadata": {
        "id": "jV_7QCxipmdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Кол-во картинок в папке: $(ls -1 \"/content/drive/My Drive/cars\" | wc -l)\""
      ],
      "metadata": {
        "id": "_YKz50OHB_xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}