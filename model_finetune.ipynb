{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mz6Pqutk-io"
      },
      "source": [
        "#Обучение модели на датасете задания\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx5Gn7TiyIGv"
      },
      "source": [
        "Я использую модель SegFormer для семантической сегментации фотографий автомобилей. Модель я буду тонко настраивать на датасете из 211 фотографий. В семантической сегментации целью для модели является пометка каждого пикселя изображения одним из предопределенных классов.\n",
        "\n",
        "Мы загружаем кодировщик модели с весами, предварительно обученными на ImageNet-1k, и тонко настраиваем его вместе с декодирующей головкой, которая начинается со случайно инициализированных весов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMYYJ7_do08a"
      },
      "outputs": [],
      "source": [
        "# Устанавливаем зависимости\n",
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9Tq2jg0z-Vl"
      },
      "outputs": [],
      "source": [
        "# Подключаем google drive к файловой системе\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "aidir = '/content/drive/My Drive/ai/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEXNfwpWsl"
      },
      "source": [
        "##Определяем пользовательский набор данных PyTorch.\n",
        "\n",
        " Каждый элемент набора данных состоит из изображения и соответствующей карты сегментации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjHHo_eLpYMa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SemanticSegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir, feature_extractor, train=True):\n",
        "        \"\"\"\n",
        "        Аргументы:\n",
        "            root_dir (строка): Корневая папка датасета который состоит из картинок и масок\n",
        "            feature_extractor (SegFormerFeatureExtractor): экстрактор признаков для подготовки картинок и масок\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
        "\n",
        "        # Считываем картинки\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "          image_file_names.extend(files)\n",
        "        self.images = sorted(image_file_names)\n",
        "\n",
        "        # Считываем маски\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "          print(files)\n",
        "          annotation_file_names.extend(files)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \"Кол-во картинок не равно кол-ву масок\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
        "        image = image.convert(\"RGB\")\n",
        "        annotation = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "\n",
        "        # 2d маска на основе 3d\n",
        "        annotation = np.array(annotation)\n",
        "        annotation_2d = np.zeros((annotation.shape[0], annotation.shape[1]), dtype=np.uint8) # высота, ширина\n",
        "\n",
        "        for id, color in id2color.items():\n",
        "            annotation_2d[(annotation == color).all(axis=-1)] = id\n",
        "\n",
        "        # случайное обрезание и заполнение изображения  и маски\n",
        "        encoded_inputs = self.feature_extractor(image, Image.fromarray(annotation_2d), return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "          encoded_inputs[k].squeeze_() # удаляем размерность группы\n",
        "\n",
        "        return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss5aoJKBnmTL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cчитываем цветовую палитру маски датасета\n",
        "color_map = pd.read_csv(aidir + 'colors.txt',\n",
        "                              sep=\" \",\n",
        "                              header=None)\n",
        "color_map.columns = [\"label_idx\", \"label\", \"R\", \"G\", \"B\"]\n",
        "color_map.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJiDidr6oJKU"
      },
      "outputs": [],
      "source": [
        "label2id = {label: id for id, label in enumerate(color_map.label)}\n",
        "id2label = {id: label for id, label in enumerate(color_map.label)}\n",
        "id2color = {id: [R,G,B] for id, (R,G,B) in enumerate(zip(color_map.R, color_map.G, color_map.B))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz8K8aFAzg0S"
      },
      "source": [
        "##Инициализация нашего датасета для обучения модели.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8GpVF2Dpkvs"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerFeatureExtractor\n",
        "\n",
        "root_dir = aidir + 'archive'\n",
        "feature_extractor = SegformerFeatureExtractor()\n",
        "\n",
        "train_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7NqpQETFcoS"
      },
      "outputs": [],
      "source": [
        "print(\"Кол-во примеров для обучения:\", len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfr3l91nrS-C"
      },
      "source": [
        "Определяем загрузчик данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4cU8hU3rZF-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "batch = next(iter(train_dataloader))\n",
        "mask = (batch[\"labels\"] != 255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R9lxUSq4mgr"
      },
      "source": [
        "## Настраиваем модель перед обучением\n",
        "\n",
        "Загружаем модель Segformer с предобученными весами nvidia/mit-b0 (самая младшая модель). Задаём id2label и label2id которые нам понадобятся при инферансе (выводе) модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81dWx1WBqIos"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
        "                                                         num_labels=5,\n",
        "                                                         id2label=id2label,\n",
        "                                                         label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgyTbnRxXvF"
      },
      "source": [
        "## Тонкая настройка модели\n",
        "\n",
        "Мы дополнительно обучаем нашу модель на датасете предоставленном к заданию с помощью оптимизатора AdamW. Для метрики обучения нашей модели используем IoU и точность на уровне пикселей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I0VhUJ5ETqq"
      },
      "outputs": [],
      "source": [
        "# Кол-во эпох\n",
        "epoch_count = 200\n",
        "# Через сколько эпох в цикле сохранять веса в папку (дефолт каждые 2 эпохи)\n",
        "epoch_checkpoint = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOdgd24YOSsw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"mean_iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHdp6-w0wDei"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
        "# Переносим модель на видеокарту\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Проверяем есть ли в нашей папке веса из предыдущих раундов обучения\n",
        "weights_dir = aidir + 'weights'\n",
        "if os.path.exists(weights_dir):\n",
        "    # Если есть, то загружаем самые свежие веса\n",
        "    weights_files = [f for f in os.listdir(weights_dir) if f.endswith('.pth')]\n",
        "else:\n",
        "    weights_files = []\n",
        "if weights_files:\n",
        "    weights_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    latest_weights_file = weights_files[-1]\n",
        "    print(f\"Загружаем веса из файла: {latest_weights_file}\")\n",
        "    if torch.cuda.is_available():\n",
        "      model.load_state_dict(torch.load(os.path.join(weights_dir, latest_weights_file)))\n",
        "    else:\n",
        "      model.load_state_dict(torch.load(os.path.join(weights_dir, latest_weights_file), map_location=torch.device('cpu')))\n",
        "    start_epoch = int(latest_weights_file.split('_')[-1].split('.')[0]) + 1\n",
        "else:\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "    start_epoch = 0\n",
        "\n",
        "model.train()\n",
        "for epoch in range(start_epoch, epoch_count):  # Цикл прохода по датасету в течении нескольких эпох\n",
        "   print(\"Epoch:\", epoch)\n",
        "   for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # Получаем входные данные\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Обнуляем параметры градиента\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Оптимизируем вперед и назад\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Вычисляем веса\n",
        "        with torch.no_grad():\n",
        "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "          predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Будем выводить потери и метрики каждые 100 пакетов\n",
        "        Выводим метрику модели\n",
        "        if idx % 100 == 0:\n",
        "\n",
        "          metrics = metric._compute(predictions=predicted.detach().cpu().numpy(),\n",
        "                                   references=labels.detach().cpu().numpy(),\n",
        "                                    num_labels=len(id2label),\n",
        "                                   ignore_index=255,\n",
        "                                   reduce_labels=False,\n",
        "          )\n",
        "          print(\"Loss:\", loss.item())\n",
        "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
        "\n",
        "   if epoch % epoch_checkpoint == 0:\n",
        "        weights_file = os.path.join(weights_dir, f'model_weights_{epoch}.pth')\n",
        "        torch.save(model.state_dict(), weights_file)\n",
        "        print(f'Сохранение весов модели в: {weights_file}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
